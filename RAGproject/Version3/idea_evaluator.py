# idea_evaluator.py
#
# Functionality：
#   Input：ideas_result generated by idea_generator
#   Output：Multidimensional score for each idea, Overall score of the idea, Rank


import json
from typing import Dict, Any, List

from llm_client import call_llm_raw
from idea_generator import generate_ideas_from_gaps
from gap_miner import mine_research_gaps


SYSTEM_PROMPT_IDEA_EVALUATOR = """You are an expert ML researcher and meta-reviewer (e.g., for NeurIPS/ICML/ACL).
Your job is to evaluate research ideas along multiple dimensions and provide a numerical score.

Evaluation dimensions (score 0–10 for each):

1. novelty: How new or original is the idea relative to current research?
2. feasibility: How feasible is the proposed method within typical academic constraints?
3. grounding: How well is the idea supported by known limitations and prior work?
4. potential_impact: If successful, how impactful would the work be?
5. clarity: How clearly defined and methodologically sound is the idea?
6. experimental_viability: Are reasonable datasets/baselines/metrics available?

You MUST output strict JSON only.
"""


def build_evaluation_prompt(ideas_result: Dict[str, Any]) -> str:
    """
    Construct a prompt by inputting the ideas JSON into the LLM and having it score each idea.
    """
    ideas_json = json.dumps(ideas_result, indent=2, ensure_ascii=False)

    prompt = f"""
You are given a JSON object 'ideas_result' with the structure:

{{
  "ideas": [
    {{
      "idea_id": "I1",
      "gap_id": "G1",
      "working_title": "...",
      "problem_statement": "...",
      "hypothesis": "...",
      "novelty": "...",
      "method_outline": "...",
      "datasets": [...],
      "baselines": [...],
      "metrics": [...],
      "risk_factors": [...]
    }},
    ...
  ]
}}

Here is the actual JSON (DO NOT repeat it in your output):
{ideas_json}

Your task:
- Evaluate each idea along the six numerical dimensions (0–10).
- Provide a short textual justification (2–4 sentences).
- Compute an overall_score = average of the six scores.
- Rank all ideas from highest to lowest score.

Output a STRICT JSON object:

{{
  "evaluations": [
    {{
      "idea_id": "I1",
      "scores": {{
        "novelty": 0,
        "feasibility": 0,
        "grounding": 0,
        "potential_impact": 0,
        "clarity": 0,
        "experimental_viability": 0
      }},
      "overall_score": 0.0,
      "justification": "2-4 sentences explaining the evaluation."
    }},
    ...
  ],
  "ranking": [
    {{
      "idea_id": "I1",
      "overall_score": 0.0
    }},
    ...
  ]
}}

Rules:
- Output valid JSON only.
- Be consistent and fair in your scoring.
"""
    return prompt


def evaluate_ideas(ideas_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Call LLM to score and sort ideas_result.
    """
    user_prompt = build_evaluation_prompt(ideas_result)
    raw_output = call_llm_raw(
        system_prompt=SYSTEM_PROMPT_IDEA_EVALUATOR,
        user_prompt=user_prompt,
        max_output_tokens=2000,
    )

    try:
        result = json.loads(raw_output)
    except json.JSONDecodeError:
        print("Failed to parse JSON in idea evaluator.")
        print("Raw output:\n", raw_output)
        raise

    return result


def pretty_print_evaluation(result: Dict[str, Any]):
    """
    Print ideas' results
    """
    print("\n=== Idea Evaluation Results ===")

    evaluations = result.get("evaluations", [])
    ranking = result.get("ranking", [])

    for e in evaluations:
        idea_id = e.get("idea_id")
        scores = e.get("scores", {})
        justification = e.get("justification", "")
        overall = e.get("overall_score")

        print(f"\n--- {idea_id} ---")
        print(f"Scores:")
        for k, v in scores.items():
            print(f"  {k}: {v}")

        print(f"\nOverall Score: {overall}")
        print(f"Justification:\n  {justification}")
        print("-" * 80)

    print("\nRanking (best → worst):")
    for r in ranking:
        print(f"  {r['idea_id']} (score={r['overall_score']})")

    print("\n================================")


def main():
    print("=== Research Idea Evaluator Demo ===")
    topic = input("Enter a topic (e.g., 'large language models'): ").strip()
    if not topic:
        topic = "large language models"

    # 1. gaps → ideas
    print("\n[1] Mining gaps and generating ideas ...")
    gaps_result = mine_research_gaps(topic, max_papers=3)
    ideas_result = generate_ideas_from_gaps(gaps_result)

    # 2. ideas → evaluation
    print("\n[2] Evaluating ideas ...")
    eval_result = evaluate_ideas(ideas_result)

    pretty_print_evaluation(eval_result)


if __name__ == "__main__":
    main()
